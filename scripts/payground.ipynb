{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic demonstration of sweeps and metrics operation.\"\"\"\n",
    "\n",
    "# %%\n",
    "# Imports, etc.\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from activation_additions import (\n",
    "    logits,\n",
    "    hyperparameter_search,\n",
    "    prompt_utils,\n",
    "    utils,\n",
    "    metrics,\n",
    "    hook_utils\n",
    ")\n",
    "\n",
    "utils.enable_ipython_reload()\n",
    "\n",
    "# Disable gradients to save memory during inference\n",
    "_ = torch.set_grad_enabled(False)\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Union,Dict\n",
    "import pandas as pd\n",
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load a model\n",
    "MODEL = HookedTransformer.from_pretrained(model_name=\"gpt2-xl\", device=\"cpu\")\n",
    "_ = MODEL.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_perplexity(model,prompt_tokens,completion_tokens):\n",
    "    metric_func=metrics.get_logprob_metric(model)\n",
    "    completed_tokens=torch.cat((prompt_tokens, completion_tokens), dim=1)\n",
    "    metric=metric_func([completed_tokens])\n",
    "    completion_logprobs=metric[\"logprob_actual_next_token\"].array[0][-completion_tokens.shape[1]:]\n",
    "    return -sum(completion_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.406886041164398"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "perplexity,  metric, completed_tokens=conditional_perplexity(MODEL,\n",
    "    MODEL.to_tokens(\"The only thing we have to fear is\"),MODEL.to_tokens(\" real fear.\")[:, 1:])\n",
    "perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 2, 4, 6, 8]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(0,10,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[49], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m grid_df\u001b[39m=\u001b[39mlayer_coefficient_gridsearch(\n\u001b[1;32m      2\u001b[0m     model\u001b[39m=\u001b[39;49mMODEL,\n\u001b[1;32m      3\u001b[0m     prompts\u001b[39m=\u001b[39;49m[\u001b[39m\"\u001b[39;49m\u001b[39mThe Most beautyful city in the world is\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m      4\u001b[0m     weighted_steering_prompts\u001b[39m=\u001b[39;49m{\u001b[39m\"\u001b[39;49m\u001b[39m Rome\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m1\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39m Paris\u001b[39;49m\u001b[39m\"\u001b[39;49m:\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m},\n\u001b[1;32m      5\u001b[0m     Layer_list\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m6\u001b[39;49m,\u001b[39m12\u001b[39;49m)),\n\u001b[1;32m      6\u001b[0m     coefficient_list\u001b[39m=\u001b[39;49m\u001b[39mlist\u001b[39;49m(\u001b[39mrange\u001b[39;49m(\u001b[39m0\u001b[39;49m,\u001b[39m10\u001b[39;49m,\u001b[39m2\u001b[39;49m)),\n\u001b[1;32m      7\u001b[0m     wanted_completions\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m Rome\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m      8\u001b[0m     unwanted_completions\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m Paris\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[48], line 28\u001b[0m, in \u001b[0;36mlayer_coefficient_gridsearch\u001b[0;34m(model, prompts, weighted_steering_prompts, Layer_list, coefficient_list, wanted_completions, unwanted_completions)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m coefficient \u001b[39min\u001b[39;00m coefficient_list:\n\u001b[1;32m     24\u001b[0m     ActAds \u001b[39m=\u001b[39m[prompt_utils\u001b[39m.\u001b[39mActivationAddition(\n\u001b[1;32m     25\u001b[0m                 coeff\u001b[39m=\u001b[39mprompt_weighting\u001b[39m*\u001b[39mcoefficient,\n\u001b[1;32m     26\u001b[0m                 act_name\u001b[39m=\u001b[39mlayer,\n\u001b[1;32m     27\u001b[0m                 prompt\u001b[39m=\u001b[39mprompt) \u001b[39mfor\u001b[39;00m prompt, prompt_weighting \u001b[39min\u001b[39;00m weighted_steering_prompts\u001b[39m.\u001b[39mitems()]\n\u001b[0;32m---> 28\u001b[0m     hook_fns\u001b[39m=\u001b[39mhook_utils\u001b[39m.\u001b[39;49mhook_fns_from_activation_additions(model, ActAds)\n\u001b[1;32m     30\u001b[0m     \u001b[39mfor\u001b[39;00m act_name, hook_fn \u001b[39min\u001b[39;00m hook_fns\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     31\u001b[0m         model\u001b[39m.\u001b[39madd_hook(act_name, hook_fn)\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:307\u001b[0m, in \u001b[0;36mhook_fns_from_activation_additions\u001b[0;34m(model, activation_additions, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Takes a list of `ActivationAddition`s and makes a single activation-modifying forward hook.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m    added in.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# Get the activation dictionary\u001b[39;00m\n\u001b[1;32m    305\u001b[0m activation_dict: Dict[\n\u001b[1;32m    306\u001b[0m     \u001b[39mstr\u001b[39m, List[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 307\u001b[0m ] \u001b[39m=\u001b[39m get_activation_dict(model, activation_additions)\n\u001b[1;32m    309\u001b[0m \u001b[39m# Make the hook functions\u001b[39;00m\n\u001b[1;32m    310\u001b[0m hook_fns: Dict[\u001b[39mstr\u001b[39m, List[Callable]] \u001b[39m=\u001b[39m hook_fns_from_act_dict(\n\u001b[1;32m    311\u001b[0m     activation_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:59\u001b[0m, in \u001b[0;36mget_activation_dict\u001b[0;34m(model, activation_additions)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Add activations for each prompt\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m activation_addition \u001b[39min\u001b[39;00m activation_additions:\n\u001b[1;32m     58\u001b[0m     activation_dict[activation_addition\u001b[39m.\u001b[39mact_name]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 59\u001b[0m         get_prompt_activations(model, activation_addition)\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m activation_dict\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:36\u001b[0m, in \u001b[0;36mget_prompt_activations\u001b[0;34m(model, activation_addition)\u001b[0m\n\u001b[1;32m     32\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(activation_addition\u001b[39m.\u001b[39mprompt)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Run the forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# ActivationCache is basically Dict[str, torch.Tensor]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m cache: ActivationCache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m     37\u001b[0m     tokens,\n\u001b[1;32m     38\u001b[0m     names_filter\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m act_name: act_name \u001b[39m==\u001b[39;49m activation_addition\u001b[39m.\u001b[39;49mact_name,\n\u001b[1;32m     39\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[39m# Return cached activations times coefficient\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m activation_addition\u001b[39m.\u001b[39mcoeff \u001b[39m*\u001b[39m cache[activation_addition\u001b[39m.\u001b[39mact_name]\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "grid_df=layer_coefficient_gridsearch(\n",
    "    model=MODEL,\n",
    "    prompts=[\"The Most beautyful city in the world is\"],\n",
    "    weighted_steering_prompts={\" Rome\":1,\" Paris\":-1},\n",
    "    Layer_list=list(range(6,12)),\n",
    "    coefficient_list=list(range(0,10,2)),\n",
    "    wanted_completions=\" Rome\",\n",
    "    unwanted_completions=\" Paris\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_coefficient_gridsearch(\n",
    "    model: HookedTransformer,\n",
    "    prompts: Union[str, List[str]],\n",
    "    weighted_steering_prompts: Dict[str, float],\n",
    "    Layer_list: List[int],\n",
    "    coefficient_list: List[float],\n",
    "    wanted_completions: Union[str, List[str]],\n",
    "    unwanted_completions: Union[str, List[str]],\n",
    ") -> pd.DataFrame:\n",
    "\n",
    "    metric_func=metrics.get_logprob_metric(model)\n",
    "\n",
    "    prompt_tokens=[model.to_tokens(prompt)for prompt in prompts]\n",
    "    wanted_completion_tokens=[model.to_tokens(wanted_completion)[:, 1:] for wanted_completion in wanted_completions]\n",
    "    unwanted_completion_tokens=[model.to_tokens(unwanted_completion)[:, 1:] for unwanted_completion in unwanted_completions]\n",
    "\n",
    "    layer_data = []\n",
    "    coefficient_data = []\n",
    "    perplexity_wanted_data = []\n",
    "    perplexity_unwanted_data = []\n",
    "\n",
    "    for layer in Layer_list:\n",
    "        for coefficient in coefficient_list:\n",
    "            ActAds =[prompt_utils.ActivationAddition(\n",
    "                        coeff=prompt_weighting*coefficient,\n",
    "                        act_name=layer,\n",
    "                        prompt=prompt) for prompt, prompt_weighting in weighted_steering_prompts.items()]\n",
    "            hook_fns=hook_utils.hook_fns_from_activation_additions(model, ActAds)\n",
    "            \n",
    "            for act_name, hook_fn in hook_fns.items():\n",
    "                model.add_hook(act_name, hook_fn)\n",
    "                \n",
    "            perplexity_on_wanted=[conditional_perplexity(model, prompt, completion) for prompt, completion in zip(prompt_tokens, wanted_completion_tokens)]\n",
    "            perplexity_on_unwanted=[conditional_perplexity(model, prompt, completion) for prompt, completion in zip(prompt_tokens, unwanted_completion_tokens)]\n",
    "            \n",
    "            model.remove_all_hook_fns()\n",
    "\n",
    "            # Append data for this layer and coefficient to the lists\n",
    "            layer_data.extend([layer] * len(prompts))\n",
    "            coefficient_data.extend([coefficient] * len(prompts))\n",
    "            perplexity_wanted_data.extend(perplexity_on_wanted)\n",
    "            perplexity_unwanted_data.extend(perplexity_on_unwanted)\n",
    "\n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        \"Layer\": layer_data,\n",
    "        \"Coefficient\": coefficient_data,\n",
    "        \"Perplexity (wanted)\": perplexity_wanted_data,\n",
    "        \"Perplexity (unwanted)\": perplexity_unwanted_data,\n",
    "        \"prompts\": prompts,\n",
    "        \"wanted_completions\":wanted_completions,\n",
    "        \"unwanted_completions\":unwanted_completions\n",
    "    })\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector=prompt_utils.weighted_prompt_superposition(MODEL,{\"hello\":1.2,\"bye\":-1.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'weighted_steering_prompts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[60], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ActAds \u001b[39m=\u001b[39m[prompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39mprompt_weighting\u001b[39m*\u001b[39mcoefficient, act_name\u001b[39m=\u001b[39mlayer,prompt\u001b[39m=\u001b[39mprompt) \u001b[39mfor\u001b[39;00m prompt, prompt_weighting \u001b[39min\u001b[39;00m weighted_steering_prompts\u001b[39m.\u001b[39mitems()]\n\u001b[1;32m      2\u001b[0m hook_fns\u001b[39m=\u001b[39mhook_utils\u001b[39m.\u001b[39mhook_fns_from_activation_additions(model, ActAds)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'weighted_steering_prompts' is not defined"
     ]
    }
   ],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=prompt_weighting*coefficient, act_name=layer,prompt=prompt) for prompt, prompt_weighting in weighted_steering_prompts.items()]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(model, ActAds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m ActAds \u001b[39m=\u001b[39m[prompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhi\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m      2\u001b[0m          prompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbye\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[0;32m----> 3\u001b[0m hook_fns\u001b[39m=\u001b[39mhook_utils\u001b[39m.\u001b[39;49mhook_fns_from_activation_additions(MODEL, ActAds)\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:307\u001b[0m, in \u001b[0;36mhook_fns_from_activation_additions\u001b[0;34m(model, activation_additions, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Takes a list of `ActivationAddition`s and makes a single activation-modifying forward hook.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m    added in.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# Get the activation dictionary\u001b[39;00m\n\u001b[1;32m    305\u001b[0m activation_dict: Dict[\n\u001b[1;32m    306\u001b[0m     \u001b[39mstr\u001b[39m, List[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 307\u001b[0m ] \u001b[39m=\u001b[39m get_activation_dict(model, activation_additions)\n\u001b[1;32m    309\u001b[0m \u001b[39m# Make the hook functions\u001b[39;00m\n\u001b[1;32m    310\u001b[0m hook_fns: Dict[\u001b[39mstr\u001b[39m, List[Callable]] \u001b[39m=\u001b[39m hook_fns_from_act_dict(\n\u001b[1;32m    311\u001b[0m     activation_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:59\u001b[0m, in \u001b[0;36mget_activation_dict\u001b[0;34m(model, activation_additions)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Add activations for each prompt\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m activation_addition \u001b[39min\u001b[39;00m activation_additions:\n\u001b[1;32m     58\u001b[0m     activation_dict[activation_addition\u001b[39m.\u001b[39mact_name]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 59\u001b[0m         get_prompt_activations(model, activation_addition)\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m activation_dict\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:36\u001b[0m, in \u001b[0;36mget_prompt_activations\u001b[0;34m(model, activation_addition)\u001b[0m\n\u001b[1;32m     32\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(activation_addition\u001b[39m.\u001b[39mprompt)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Run the forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# ActivationCache is basically Dict[str, torch.Tensor]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m cache: ActivationCache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m     37\u001b[0m     tokens,\n\u001b[1;32m     38\u001b[0m     names_filter\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m act_name: act_name \u001b[39m==\u001b[39;49m activation_addition\u001b[39m.\u001b[39;49mact_name,\n\u001b[1;32m     39\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[39m# Return cached activations times coefficient\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m activation_addition\u001b[39m.\u001b[39mcoeff \u001b[39m*\u001b[39m cache[activation_addition\u001b[39m.\u001b[39mact_name]\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "ActAds =[prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\"),\n",
    "         prompt_utils.ActivationAddition(coeff=-1, act_name=1,prompt=\"bye\")]\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL, ActAds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'list' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m ActAds\u001b[39m=\u001b[39mprompt_utils\u001b[39m.\u001b[39mActivationAddition(coeff\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, act_name\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,prompt\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhi\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m hook_utils\u001b[39m.\u001b[39;49mhook_fns_from_activation_additions(MODEL, [ActAds])\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:307\u001b[0m, in \u001b[0;36mhook_fns_from_activation_additions\u001b[0;34m(model, activation_additions, **kwargs)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Takes a list of `ActivationAddition`s and makes a single activation-modifying forward hook.\u001b[39;00m\n\u001b[1;32m    291\u001b[0m \n\u001b[1;32m    292\u001b[0m \u001b[39margs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m    added in.\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    304\u001b[0m \u001b[39m# Get the activation dictionary\u001b[39;00m\n\u001b[1;32m    305\u001b[0m activation_dict: Dict[\n\u001b[1;32m    306\u001b[0m     \u001b[39mstr\u001b[39m, List[Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]]\n\u001b[0;32m--> 307\u001b[0m ] \u001b[39m=\u001b[39m get_activation_dict(model, activation_additions)\n\u001b[1;32m    309\u001b[0m \u001b[39m# Make the hook functions\u001b[39;00m\n\u001b[1;32m    310\u001b[0m hook_fns: Dict[\u001b[39mstr\u001b[39m, List[Callable]] \u001b[39m=\u001b[39m hook_fns_from_act_dict(\n\u001b[1;32m    311\u001b[0m     activation_dict, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    312\u001b[0m )\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:59\u001b[0m, in \u001b[0;36mget_activation_dict\u001b[0;34m(model, activation_additions)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[39m# Add activations for each prompt\u001b[39;00m\n\u001b[1;32m     57\u001b[0m \u001b[39mfor\u001b[39;00m activation_addition \u001b[39min\u001b[39;00m activation_additions:\n\u001b[1;32m     58\u001b[0m     activation_dict[activation_addition\u001b[39m.\u001b[39mact_name]\u001b[39m.\u001b[39mappend(\n\u001b[0;32m---> 59\u001b[0m         get_prompt_activations(model, activation_addition)\n\u001b[1;32m     60\u001b[0m     )\n\u001b[1;32m     62\u001b[0m \u001b[39mreturn\u001b[39;00m activation_dict\n",
      "File \u001b[0;32m~/activation_additions/activation_additions/hook_utils.py:36\u001b[0m, in \u001b[0;36mget_prompt_activations\u001b[0;34m(model, activation_addition)\u001b[0m\n\u001b[1;32m     32\u001b[0m     tokens \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mto_tokens(activation_addition\u001b[39m.\u001b[39mprompt)\n\u001b[1;32m     34\u001b[0m \u001b[39m# Run the forward pass\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m# ActivationCache is basically Dict[str, torch.Tensor]\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m cache: ActivationCache \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m     37\u001b[0m     tokens,\n\u001b[1;32m     38\u001b[0m     names_filter\u001b[39m=\u001b[39;49m\u001b[39mlambda\u001b[39;49;00m act_name: act_name \u001b[39m==\u001b[39;49m activation_addition\u001b[39m.\u001b[39;49mact_name,\n\u001b[1;32m     39\u001b[0m )[\u001b[39m1\u001b[39m]\n\u001b[1;32m     41\u001b[0m \u001b[39m# Return cached activations times coefficient\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[39mreturn\u001b[39;00m activation_addition\u001b[39m.\u001b[39mcoeff \u001b[39m*\u001b[39m cache[activation_addition\u001b[39m.\u001b[39mact_name]\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:429\u001b[0m, in \u001b[0;36mHookedTransformer.run_with_cache\u001b[0;34m(self, return_cache_object, remove_batch_dim, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mrun_with_cache\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mmodel_args, return_cache_object\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, remove_batch_dim\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    415\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m     Union[ActivationCache, Dict[\u001b[39mstr\u001b[39m, torch\u001b[39m.\u001b[39mTensor]],\n\u001b[1;32m    423\u001b[0m ]:\n\u001b[1;32m    424\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[39m    Wrapper around run_with_cache in HookedRootModule. If return_cache_object is True, this will return an\u001b[39;00m\n\u001b[1;32m    426\u001b[0m \u001b[39m    ActivationCache object, with a bunch of useful HookedTransformer specific methods, otherwise it will return a\u001b[39;00m\n\u001b[1;32m    427\u001b[0m \u001b[39m    dictionary of activations as in HookedRootModule.\u001b[39;00m\n\u001b[1;32m    428\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 429\u001b[0m     out, cache_dict \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mrun_with_cache(\n\u001b[1;32m    430\u001b[0m         \u001b[39m*\u001b[39;49mmodel_args, remove_batch_dim\u001b[39m=\u001b[39;49mremove_batch_dim, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     \u001b[39mif\u001b[39;00m return_cache_object:\n\u001b[1;32m    433\u001b[0m         cache \u001b[39m=\u001b[39m ActivationCache(\n\u001b[1;32m    434\u001b[0m             cache_dict, \u001b[39mself\u001b[39m, has_batch_dim\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m remove_batch_dim\n\u001b[1;32m    435\u001b[0m         )\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:457\u001b[0m, in \u001b[0;36mHookedRootModule.run_with_cache\u001b[0;34m(self, names_filter, device, remove_batch_dim, incl_bwd, reset_hooks_end, clear_contexts, *model_args, **model_kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m cache_dict, fwd, bwd \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_caching_hooks(\n\u001b[1;32m    448\u001b[0m     names_filter, incl_bwd, device, remove_batch_dim\u001b[39m=\u001b[39mremove_batch_dim\n\u001b[1;32m    449\u001b[0m )\n\u001b[1;32m    451\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhooks(\n\u001b[1;32m    452\u001b[0m     fwd_hooks\u001b[39m=\u001b[39mfwd,\n\u001b[1;32m    453\u001b[0m     bwd_hooks\u001b[39m=\u001b[39mbwd,\n\u001b[1;32m    454\u001b[0m     reset_hooks_end\u001b[39m=\u001b[39mreset_hooks_end,\n\u001b[1;32m    455\u001b[0m     clear_contexts\u001b[39m=\u001b[39mclear_contexts,\n\u001b[1;32m    456\u001b[0m ):\n\u001b[0;32m--> 457\u001b[0m     model_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\u001b[39m*\u001b[39;49mmodel_args, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs)\n\u001b[1;32m    458\u001b[0m     \u001b[39mif\u001b[39;00m incl_bwd:\n\u001b[1;32m    459\u001b[0m         model_out\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/HookedTransformer.py:358\u001b[0m, in \u001b[0;36mHookedTransformer.forward\u001b[0;34m(self, input, return_type, loss_per_token, prepend_bos, stop_at_layer, past_kv_cache)\u001b[0m\n\u001b[1;32m    353\u001b[0m     \u001b[39mif\u001b[39;00m shortformer_pos_embed \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    354\u001b[0m         shortformer_pos_embed \u001b[39m=\u001b[39m shortformer_pos_embed\u001b[39m.\u001b[39mto(\n\u001b[1;32m    355\u001b[0m             devices\u001b[39m.\u001b[39mget_device_for_block_index(i, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcfg)\n\u001b[1;32m    356\u001b[0m         )\n\u001b[0;32m--> 358\u001b[0m     residual \u001b[39m=\u001b[39m block(\n\u001b[1;32m    359\u001b[0m         residual,\n\u001b[1;32m    360\u001b[0m         past_kv_cache_entry\u001b[39m=\u001b[39;49mpast_kv_cache[i]\n\u001b[1;32m    361\u001b[0m         \u001b[39mif\u001b[39;49;00m past_kv_cache \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m    362\u001b[0m         \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,  \u001b[39m# Cache contains a list of HookedTransformerKeyValueCache objects, one for each block\u001b[39;49;00m\n\u001b[1;32m    363\u001b[0m         shortformer_pos_embed\u001b[39m=\u001b[39;49mshortformer_pos_embed,\n\u001b[1;32m    364\u001b[0m     )  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    366\u001b[0m \u001b[39mif\u001b[39;00m stop_at_layer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    367\u001b[0m     \u001b[39m# When we stop at an early layer, we end here rather than doing further computation\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/components.py:893\u001b[0m, in \u001b[0;36mTransformerBlock.forward\u001b[0;34m(self, resid_pre, shortformer_pos_embed, past_kv_cache_entry)\u001b[0m\n\u001b[1;32m    875\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    876\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    877\u001b[0m     resid_pre: Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    881\u001b[0m     past_kv_cache_entry: Optional[HookedTransformerKeyValueCacheEntry] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    882\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Float[torch\u001b[39m.\u001b[39mTensor, \u001b[39m\"\u001b[39m\u001b[39mbatch pos d_model\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[1;32m    883\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"A single Transformer block.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \n\u001b[1;32m    885\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[39m        _type_: _description_\u001b[39;00m\n\u001b[1;32m    892\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 893\u001b[0m     resid_pre \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhook_resid_pre(resid_pre)  \u001b[39m# [batch, pos, d_model]\u001b[39;00m\n\u001b[1;32m    895\u001b[0m     query_input \u001b[39m=\u001b[39m resid_pre\n\u001b[1;32m    896\u001b[0m     key_input \u001b[39m=\u001b[39m resid_pre\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/torch/nn/modules/module.py:1215\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1213\u001b[0m \u001b[39mif\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks:\n\u001b[1;32m   1214\u001b[0m     \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m (\u001b[39m*\u001b[39m_global_forward_hooks\u001b[39m.\u001b[39mvalues(), \u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[0;32m-> 1215\u001b[0m         hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39;49m, \u001b[39minput\u001b[39;49m, result)\n\u001b[1;32m   1216\u001b[0m         \u001b[39mif\u001b[39;00m hook_result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1217\u001b[0m             result \u001b[39m=\u001b[39m hook_result\n",
      "File \u001b[0;32m~/miniconda/envs/my_env/lib/python3.10/site-packages/transformer_lens/hook_points.py:67\u001b[0m, in \u001b[0;36mHookPoint.add_hook.<locals>.full_hook\u001b[0;34m(module, module_input, module_output)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfull_hook\u001b[39m(module, module_input, module_output):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m hook(module_output, hook\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'list' object is not callable"
     ]
    }
   ],
   "source": [
    "ActAds=prompt_utils.ActivationAddition(coeff=1, act_name=1,prompt=\"hi\")\n",
    "hook_utils.hook_fns_from_activation_additions(MODEL, [ActAds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=list(range(20))\n",
    "coefficents=list(range(20))\n",
    "activation_additions=[]\n",
    "\n",
    "for layer in layers:\n",
    "    vector_layer=get_x_vector_preset(prompt1=\" love\", prompt2=\" hate\", coeff=1, act_name=layer)\n",
    "    for coefficent in coefficents:\n",
    "        vector=deepcopy(vector_layer)\n",
    "        for i in range(len(vector)):\n",
    "            vector[i].coeff=coefficent*vector[i].coeff\n",
    "        activation_additions.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">probs</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">logprobs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50247</th>\n",
       "      <th>50248</th>\n",
       "      <th>50249</th>\n",
       "      <th>50250</th>\n",
       "      <th>50251</th>\n",
       "      <th>50252</th>\n",
       "      <th>50253</th>\n",
       "      <th>50254</th>\n",
       "      <th>50255</th>\n",
       "      <th>50256</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>4.937604e-07</td>\n",
       "      <td>2.832087e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.916537e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.978655</td>\n",
       "      <td>-15.196625</td>\n",
       "      <td>-18.414135</td>\n",
       "      <td>-14.930026</td>\n",
       "      <td>-17.293758</td>\n",
       "      <td>-15.18249</td>\n",
       "      <td>-14.840092</td>\n",
       "      <td>-16.476681</td>\n",
       "      <td>-15.081723</td>\n",
       "      <td>-11.901134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows  100514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        probs                                                                \\\n",
       "        0         1             2             3         4             5       \n",
       "pos                                                                           \n",
       "5    0.000006  0.000007  4.937604e-07  2.832087e-07  0.000001  3.916537e-07   \n",
       "\n",
       "                                             ...   logprobs             \\\n",
       "        6         7         8         9      ...      50247      50248   \n",
       "pos                                          ...                         \n",
       "5    0.000006  0.000007  0.000008  0.000004  ... -15.978655 -15.196625   \n",
       "\n",
       "                                                                      \\\n",
       "         50249      50250      50251     50252      50253      50254   \n",
       "pos                                                                    \n",
       "5   -18.414135 -14.930026 -17.293758 -15.18249 -14.840092 -16.476681   \n",
       "\n",
       "                           \n",
       "         50255      50256  \n",
       "pos                        \n",
       "5   -15.081723 -11.901134  \n",
       "\n",
       "[1 rows x 100514 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_additions[2]\n",
    "logits.get_token_probs(MODEL,prompts,activation_additions[1],return_positions_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_x_vector_preset = partial(prompt_utils.get_x_vector, pad_method=\"tokens_right\",\n",
    "                              model=MODEL,\n",
    "                              custom_pad_id=MODEL.to_single_token(\" \"))\n",
    "activation_additions=get_x_vector_preset(prompt1=\" love\", prompt2=\" hate\", coeff=1, act_name=6)\n",
    "prompts=\"I love you. The\"\n",
    "return_positions_above=len(MODEL.tokenizer.encode(prompts))\n",
    "matrix=[]\n",
    "for activation_addition in [activation_additions]:\n",
    "    print(8)\n",
    "    a=logits.get_token_probs(MODEL,prompts,activation_addition,return_positions_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        probs                                                                \\\n",
       "         0         1             2             3         4             5       \n",
       " pos                                                                           \n",
       " 5    0.000005  0.000007  4.419383e-07  2.559607e-07  0.000001  3.399807e-07   \n",
       " \n",
       "                                              ...   logprobs             \\\n",
       "         6         7         8         9      ...      50247      50248   \n",
       " pos                                          ...                         \n",
       " 5    0.000006  0.000006  0.000008  0.000004  ... -16.132639 -15.316798   \n",
       " \n",
       "                                                                        \\\n",
       "          50249      50250      50251      50252      50253      50254   \n",
       " pos                                                                     \n",
       " 5   -18.390577 -14.896774 -17.277262 -15.282727 -14.884817 -16.464771   \n",
       " \n",
       "                            \n",
       "          50255      50256  \n",
       " pos                        \n",
       " 5   -15.093022 -11.976322  \n",
       " \n",
       " [1 rows x 100514 columns],\n",
       "         probs                                                                \\\n",
       "         0         1             2             3         4             5       \n",
       " pos                                                                           \n",
       " 5    0.000005  0.000007  4.419383e-07  2.559607e-07  0.000001  3.399807e-07   \n",
       " \n",
       "                                              ...   logprobs             \\\n",
       "         6         7         8         9      ...      50247      50248   \n",
       " pos                                          ...                         \n",
       " 5    0.000006  0.000006  0.000008  0.000004  ... -16.132639 -15.316798   \n",
       " \n",
       "                                                                        \\\n",
       "          50249      50250      50251      50252      50253      50254   \n",
       " pos                                                                     \n",
       " 5   -18.390577 -14.896774 -17.277262 -15.282727 -14.884817 -16.464771   \n",
       " \n",
       "                            \n",
       "          50255      50256  \n",
       " pos                        \n",
       " 5   -15.093022 -11.976322  \n",
       " \n",
       " [1 rows x 100514 columns]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationAddition(tensor([50256,  1842], device='cuda:0'), 1, blocks.6.hook_resid_pre)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_additions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'act_name', 'coeff', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "print(dir(activation_additions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_additions[0]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
