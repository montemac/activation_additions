{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Basic demonstration of sweeps and metrics operation.\"\"\"\n",
    "\n",
    "# %%\n",
    "# Imports, etc.\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import torch\n",
    "\n",
    "\n",
    "from transformer_lens import HookedTransformer\n",
    "\n",
    "from activation_additions import (\n",
    "    logits,\n",
    "    hyperparameter_search,\n",
    "    prompt_utils,\n",
    "    utils,\n",
    "    metrics,\n",
    "    hook_utils\n",
    ")\n",
    "\n",
    "utils.enable_ipython_reload()\n",
    "\n",
    "# Disable gradients to save memory during inference\n",
    "_ = torch.set_grad_enabled(False)\n",
    "\n",
    "from copy import deepcopy\n",
    "from typing import List, Union,Dict\n",
    "import pandas as pd\n",
    "from transformer_lens.utils import get_act_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hook_embed'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_act_name(name=\"embed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ActAds = prompt_utils.ActivationAddition(\n",
    "    coeff=1.0,\n",
    "    act_name=1,\n",
    "    prompt='Bob went')\n",
    "hook_fns=hook_utils.hook_fns_from_activation_additions(MODEL,[ActAds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "for act_name, hook_fn in hook_fns.items():\n",
    "    MODEL.add_hook(act_name, hook_fn)\n",
    "MODEL.remove_all_hook_fns()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using pad_token, but it is not set yet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-xl into HookedTransformer\n",
      "Moving model to device:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Load a model\n",
    "MODEL = HookedTransformer.from_pretrained(model_name=\"gpt2-xl\", device=\"cpu\")\n",
    "_ = MODEL.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_func=metrics.get_logprob_metric(MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens1=MODEL.to_tokens(\"Hello, my name is John\")\n",
    "tokens2=MODEL.to_tokens(\"Wntworth\")[:, 1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([50256, 15496,    11,   616,  1438,   318,  1757], device='cuda:0')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Hello, my name is John'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.tokenizer.decode(tokens1[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496,    11,   616,  1438,   318,  1757, 50256,    54,   429,\n",
       "          9268]], device='cuda:0')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((tokens1, tokens2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens2.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|endoftext|>Hello, my name is John<|endoftext|>Wntworth'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.tokenizer.decode(torch.cat((tokens1, tokens2), dim=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[50256, 15496,    11,   616,  1438,   318,  1757]], device='cuda:0')]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[50256, 15496,    11,   616,  1438,   318,  1757,    54,   429,  9268]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((tokens1, tokens2), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>logprob_actual_next_token</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-7.004738, -1.7059618, -2.5899856, -0.1306995...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           logprob_actual_next_token\n",
       "0  [-7.004738, -1.7059618, -2.5899856, -0.1306995..."
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric_func([torch.cat((tokens1, tokens2), dim=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric=metric_func([tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-7.0047364 , -1.705964  , -2.5899823 , -0.13069953, -0.0268248 ,\n",
       "       -4.583589  ], dtype=float32)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric[\"logprob_actual_next_token\"].array[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13069953, -0.0268248 , -4.583589  ], dtype=float32)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric[\"logprob_actual_next_token\"].array[0][-tokens2.shape[1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [-7.0047364, -1.705964, -2.5899823, -0.1306995...\n",
      "Name: logprob_actual_next_token, dtype: object\n"
     ]
    }
   ],
   "source": [
    "for row in metric:\n",
    "    print(metric[row])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_steering_prompts={\"love\":-1,\"hate\":1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([-1, 1])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_steering_prompts.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional_perplexity(model,prompt_tokens,completion_tokens):\n",
    "    metric_func=metrics.get_logprob_metric(model)\n",
    "    metric=metric_func([torch.cat((prompt_tokens, completion_tokens), dim=1)])\n",
    "    completion_logprobs=metric[\"logprob_actual_next_token\"].array[0][-completion_tokens.shape[1]:]\n",
    "    return -sum(completion_logprobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26.50157356262207"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_perplexity(MODEL,MODEL.to_tokens(\"Hello, my name is\"),MODEL.to_tokens(\" Gabe\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3253130084.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[28], line 9\u001b[0;36m\u001b[0m\n\u001b[0;31m    ) -> pd.DataFrame:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def layer_coefficient_gridsearch(\n",
    "    model: HookedTransformer,\n",
    "    prompts: Union[str, List[str]],\n",
    "    weighted_steering_prompts: Dict[str, float],\n",
    "    Layer_list: List[int],\n",
    "    coefficient_list: List[float],\n",
    "    wanted_completions: Union[str, List[str]],\n",
    "    unwanted_completions: Union[str, List[str]],\n",
    ") -> pd.DataFrame:\n",
    "    metric_func=metrics.get_logprob_metric(model)\n",
    "    prompt_tokens=[model.to_tokens(prompt)for prompt in prompts]\n",
    "    wanted_completion_tokens=[model.to_tokens(wanted_completion)[:, 1:] for wanted_completion in wanted_completions]\n",
    "    unwanted_completion_tokens=[model.to_tokens(unwanted_completion)[:, 1:] for unwanted_completion in unwanted_completions]\n",
    "    for layer in Layer_list:\n",
    "        for coefficent in coefficient_list:\n",
    "            ActAds =[prompt_utils.ActivationAddition(\n",
    "                        coeff=prompt_wighting*coefficent,\n",
    "                        act_name=layer,\n",
    "                        prompt=prompt) for prompt, prompt_wighting in weighted_steering_prompts.items()]\n",
    "            hook_fns=hook_utils.hook_fns_from_activation_additions(model,ActAds)\n",
    "            for act_name, hook_fn in hook_fns.items():\n",
    "                model.add_hook(act_name, hook_fn)\n",
    "            model.remove_all_hook_fns()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector=prompt_utils.weighted_prompt_superposition(MODEL,{\"hello\":1.2,\"bye\":-1.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ActivationAddition(hello, 1.2, hook_embed),\n",
       " ActivationAddition(bye, -1.2, hook_embed),\n",
       " ActivationAddition(tensor([64, 64], dtype=torch.int32), -1.0, hook_embed)]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "steering_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers=list(range(20))\n",
    "coefficents=list(range(20))\n",
    "activation_additions=[]\n",
    "\n",
    "for layer in layers:\n",
    "    vector_layer=get_x_vector_preset(prompt1=\" love\", prompt2=\" hate\", coeff=1, act_name=layer)\n",
    "    for coefficent in coefficents:\n",
    "        vector=deepcopy(vector_layer)\n",
    "        for i in range(len(vector)):\n",
    "            vector[i].coeff=coefficent*vector[i].coeff\n",
    "        activation_additions.append(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"10\" halign=\"left\">probs</th>\n",
       "      <th>...</th>\n",
       "      <th colspan=\"10\" halign=\"left\">logprobs</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>50247</th>\n",
       "      <th>50248</th>\n",
       "      <th>50249</th>\n",
       "      <th>50250</th>\n",
       "      <th>50251</th>\n",
       "      <th>50252</th>\n",
       "      <th>50253</th>\n",
       "      <th>50254</th>\n",
       "      <th>50255</th>\n",
       "      <th>50256</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>4.937604e-07</td>\n",
       "      <td>2.832087e-07</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>3.916537e-07</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>...</td>\n",
       "      <td>-15.978655</td>\n",
       "      <td>-15.196625</td>\n",
       "      <td>-18.414135</td>\n",
       "      <td>-14.930026</td>\n",
       "      <td>-17.293758</td>\n",
       "      <td>-15.18249</td>\n",
       "      <td>-14.840092</td>\n",
       "      <td>-16.476681</td>\n",
       "      <td>-15.081723</td>\n",
       "      <td>-11.901134</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 100514 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        probs                                                                \\\n",
       "        0         1             2             3         4             5       \n",
       "pos                                                                           \n",
       "5    0.000006  0.000007  4.937604e-07  2.832087e-07  0.000001  3.916537e-07   \n",
       "\n",
       "                                             ...   logprobs             \\\n",
       "        6         7         8         9      ...      50247      50248   \n",
       "pos                                          ...                         \n",
       "5    0.000006  0.000007  0.000008  0.000004  ... -15.978655 -15.196625   \n",
       "\n",
       "                                                                      \\\n",
       "         50249      50250      50251     50252      50253      50254   \n",
       "pos                                                                    \n",
       "5   -18.414135 -14.930026 -17.293758 -15.18249 -14.840092 -16.476681   \n",
       "\n",
       "                           \n",
       "         50255      50256  \n",
       "pos                        \n",
       "5   -15.081723 -11.901134  \n",
       "\n",
       "[1 rows x 100514 columns]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_additions[2]\n",
    "logits.get_token_probs(MODEL,prompts,activation_additions[1],return_positions_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "get_x_vector_preset = partial(prompt_utils.get_x_vector, pad_method=\"tokens_right\",\n",
    "                              model=MODEL,\n",
    "                              custom_pad_id=MODEL.to_single_token(\" \"))\n",
    "activation_additions=get_x_vector_preset(prompt1=\" love\", prompt2=\" hate\", coeff=1, act_name=6)\n",
    "prompts=\"I love you. The\"\n",
    "return_positions_above=len(MODEL.tokenizer.encode(prompts))\n",
    "matrix=[]\n",
    "for activation_addition in [activation_additions]:\n",
    "    print(8)\n",
    "    a=logits.get_token_probs(MODEL,prompts,activation_addition,return_positions_above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[        probs                                                                \\\n",
       "         0         1             2             3         4             5       \n",
       " pos                                                                           \n",
       " 5    0.000005  0.000007  4.419383e-07  2.559607e-07  0.000001  3.399807e-07   \n",
       " \n",
       "                                              ...   logprobs             \\\n",
       "         6         7         8         9      ...      50247      50248   \n",
       " pos                                          ...                         \n",
       " 5    0.000006  0.000006  0.000008  0.000004  ... -16.132639 -15.316798   \n",
       " \n",
       "                                                                        \\\n",
       "          50249      50250      50251      50252      50253      50254   \n",
       " pos                                                                     \n",
       " 5   -18.390577 -14.896774 -17.277262 -15.282727 -14.884817 -16.464771   \n",
       " \n",
       "                            \n",
       "          50255      50256  \n",
       " pos                        \n",
       " 5   -15.093022 -11.976322  \n",
       " \n",
       " [1 rows x 100514 columns],\n",
       "         probs                                                                \\\n",
       "         0         1             2             3         4             5       \n",
       " pos                                                                           \n",
       " 5    0.000005  0.000007  4.419383e-07  2.559607e-07  0.000001  3.399807e-07   \n",
       " \n",
       "                                              ...   logprobs             \\\n",
       "         6         7         8         9      ...      50247      50248   \n",
       " pos                                          ...                         \n",
       " 5    0.000006  0.000006  0.000008  0.000004  ... -16.132639 -15.316798   \n",
       " \n",
       "                                                                        \\\n",
       "          50249      50250      50251      50252      50253      50254   \n",
       " pos                                                                     \n",
       " 5   -18.390577 -14.896774 -17.277262 -15.282727 -14.884817 -16.464771   \n",
       " \n",
       "                            \n",
       "          50255      50256  \n",
       " pos                        \n",
       " 5   -15.093022 -11.976322  \n",
       " \n",
       " [1 rows x 100514 columns]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationAddition(tensor([50256,  1842], device='cuda:0'), 1, blocks.6.hook_resid_pre)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activation_additions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__annotations__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'act_name', 'coeff', 'tokens']\n"
     ]
    }
   ],
   "source": [
    "print(dir(activation_additions[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_additions[0]=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
