# ---
# jupyter:
#   jupytext:
#     cell_metadata_filter: -all
#     custom_cell_magics: kql
#     formats: ipynb,py:percent
#     text_representation:
#       extension: .py
#       format_name: percent
#       format_version: '1.3'
#       jupytext_version: 1.14.5
#   kernelspec:
#     display_name: AlexT_norms
#     language: python
#     name: python3
# ---

# %% [markdown]
# # Exponential growth of residual stream norms

# %%
try:
    import algebraic_value_editing
except ImportError:
    commit = "15bcf55"  # Stable commit
    get_ipython().run_line_magic(  # type: ignore
        magic_name="pip",
        line=(
            "install -U"
            f" git+https://github.com/montemac/algebraic_value_editing.git@{commit}"
        ),
    )


# %%
import torch
import pandas as pd
from typing import List, Dict

from transformer_lens.HookedTransformer import HookedTransformer

from algebraic_value_editing import hook_utils, prompt_utils
from algebraic_value_editing.prompt_utils import RichPrompt


# %%
device: str = "cpu"
model_name = "gpt2-xl"
model: HookedTransformer = HookedTransformer.from_pretrained(model_name, device="cpu")
_ = model.to(device)

_ = torch.set_grad_enabled(False)
torch.manual_seed(0)  # For reproducibility


# %% [markdown]
# Let's examine what the residual stream magnitudes tend to be, by taking the Frobenius
# norm of the residual stream at each sequence position. We'll do this for
# a range of prompts at a range of locations in the forward pass. (The
# downloaded prompts were mostly generated by GPT-4.)

# %%
import requests

url = "https://raw.githubusercontent.com/montemac/algebraic_value_editing/cb6b1a42493a385ca02e7b9e6bbcb9bff9d006dc/scripts/prompts.txt"  # Cached at a commit to prevent changing results

response = requests.get(url)

if response.status_code == 200:
    # If the request is successful, split the content by line breaks to create a list of strings
    prompts = response.text.splitlines()
    print(f"Downloaded {len(prompts)} prompts")
else:
    raise Exception(
        f"Failed to download the file: {response.status_code} -" f" {response.reason}"
    )


# %%
DF_COLS: List[str] = [
    "Prompt",
    "Activation Location",
    "Activation Name",
    "Magnitude",
]

sampling_kwargs: Dict[str, float] = {
    "temperature": 1.0,
    "top_p": 0.3,
    "freq_penalty": 1.0,
}

num_layers: int = model.cfg.n_layers


# %% [markdown]
# ## Residual stream magnitudes increase exponentially with layer number
# As the forward pass progresses through the network, the residual
# stream tends to increase in magnitude in an exponential fashion. This
# is easily visible in the histogram below, which shows the distribution
# of residual stream magnitudes for each layer of the network. The activation
# distribution translates by an almost constant factor each 6 layers,
# and the x-axis (magnitude) is log-scale, so magnitude apparently
# increases exponentially with layer number.*
#
# (Intriguingly, there are a few outlier residual streams which have
# magnitude over an order of magnitude larger than the rest.)
#
# Alex's first guess for the exponential magnitude increase was: Each OV circuit is a linear function of the
# residual stream given a fixed attention pattern. Then you add the head
# OV outputs back into a residual stream, which naively doubles the
# magnitude assuming the OV outputs have similar norm to the input
# residual stream. The huge problem with this explanation is layernorm,
# which is applied to the inputs to the attention and MLP layers. This
# should basically whiten the input to the OV circuits if the gain
# parameters are close to 1.
#
# \* Stefan Heimersheim previously noticed this phenomenon in GPT2-small.

# %%
import plotly.express as px
import plotly.graph_objects as go
import numpy as np


def magnitude_histogram(df: pd.DataFrame, title="Residual Stream Magnitude by Layer Number",
    xaxis_title="log10 Residual Stream norm", yaxis_title="Percentage of residual streams") -> go.Figure:
    """Plot a histogram of the residual stream magnitudes for each layer
    of the network."""
    assert (
        "Magnitude" in df.columns
    ), "Dataframe must have a 'Magnitude' column"

    df["LogMagnitude"] = np.log10(df["Magnitude"])

    # Get the number of unique activation locations
    num_unique_activation_locations = df["Activation Location"].nunique()

    # Generate a color list that is long enough to accommodate all unique activation locations
    extended_rainbow = (
        px.colors.sequential.Rainbow * num_unique_activation_locations
    )
    color_list = extended_rainbow[:num_unique_activation_locations]

    fig = px.histogram(
        df,
        x="LogMagnitude",
        color="Activation Location",
        marginal="rug",
        histnorm="percent",
        nbins=100,
        opacity=0.5,
        barmode="overlay",
        color_discrete_sequence=color_list,
    )

    fig.update_layout(
        legend_title_text="After Layer Number",
        title=title,
        xaxis_title=xaxis_title,
        yaxis_title=yaxis_title,
    )

    return fig


# %%
# Create an empty dataframe with the required columns
prompt_df = pd.DataFrame(columns=DF_COLS)

from algebraic_value_editing import prompt_utils

# Loop through activation locations and prompts
activation_locations_8: List[int] = torch.arange(
    0, num_layers, num_layers // 8
).tolist()
for act_loc in activation_locations_8:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)
    for prompt in prompts:
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=prompt, act_name=act_name
        ).cpu()

        # Create a new dataframe row with the current data
        row = pd.DataFrame(
            {
                "Prompt": prompt,
                "Activation Location": act_loc,
                "Activation Name": act_name,
                "Magnitude": mags[1:], #remove BOS
            }
        )

        # Append the new row to the dataframe
        prompt_df = pd.concat([prompt_df, row], ignore_index=True)


# %%
fig: go.Figure = magnitude_histogram(prompt_df)
fig.show()

# %% [markdown]
# In GPT2-XL, the fast magnitude gain
# occurs in the first 7 layers. Let's find out where.

# %%
activation_locations: List[int] = list(range(7))
first_7_df = pd.DataFrame(columns=DF_COLS)

for act_loc in activation_locations:
    prefixes = ["pre", "mid", "post"] if act_loc == 0 else ["mid", "post"]
    for prefix in prefixes:
        act_name = f"blocks.{act_loc}.hook_resid_{prefix}"
        for prompt in prompts:
            mags: torch.Tensor = hook_utils.prompt_magnitudes(
                model=model, prompt=prompt, act_name=act_name
            ).cpu()
            loc_delta = 0 if prefix == "pre" else 0.5 if prefix == "mid" else 1
            # Create a new dataframe row with the current data
            row = pd.DataFrame(
                {
                    "Prompt": prompt,
                    "Activation Location": act_loc + loc_delta,
                    "Activation Name": act_name,
                    "Magnitude": mags[1:],
                }
            )

            # Append the new row to the dataframe
            first_7_df = pd.concat([first_7_df, row], ignore_index=True)

# %%
fig: go.Figure = magnitude_histogram(first_7_df)
fig.show()

# %% [markdown]
# Most of the jump happens after the 0th layer in the transformer, and
# a smaller jump happens between the 1st and 2nd layers.

# %% [markdown]
# ## Test this on other models

# %% [markdown]
# ### Stefan's original findings
#

# %%
import matplotlib.pyplot as plt

cache = model.run_with_cache(prompts)[1]
stds = []
meanstds = []
for layer in range(model.cfg.n_layers):
    std = cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1)
    stds.append(std)
    stds_non_BOS_non_PAD = []
    for batch in range(len(prompts)):
        prompt_len = len(model.to_str_tokens(prompts[batch]))
        #print(prompt_len, model.to_str_tokens(prompts[batch]))
        for pos in range(1, prompt_len):
            stds_non_BOS_non_PAD.append(std[batch, pos].item())
    meanstds.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())

plt.figure(figsize=(10,5))
plt.plot(range(model.cfg.n_layers), meanstds, marker="o", label=model_name)
plt.title("Mean Standard Deviation of Residual Stream activations after layer $N$")
plt.xlabel("Layer Index $N$")
plt.ylabel("Mean Standard Deviation")
plt.legend()
plt.savefig(f"stds_{model_name}.png")
plt.show()

# %% [markdown]
# ### Version with BOS and Padding

# %%
import matplotlib.pyplot as plt

cache = model.run_with_cache(prompts)[1]
stds = []
meanstds = []
for layer in range(model.cfg.n_layers):
    std = cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1)
    stds.append(std[:, 0:])
    meanstds.append(std[:, 1:].mean(dim=(0,1)).item())

plt.figure(figsize=(10,5))
plt.plot(range(model.cfg.n_layers), meanstds, marker="o", label=model_name)
plt.title("Mean Standard Deviation of Residual Stream activations after layer $N$")
plt.xlabel("$N$")
plt.ylabel("Mean Standard Deviation")
plt.legend()
plt.savefig(f"stds_{model_name}.png")
plt.show()

n_batch = len(stds[0])
n_pos = len(stds[0][0])
n_layers = len(stds)
plt.figure(figsize=(10,5))
for batch in range(n_batch):
    for pos in range(n_pos):
        #print(f"Batch {batch}, Position {pos}")
        color = plt.cm.viridis(pos/n_pos)
        plt.plot(range(n_layers), [stds[i][batch][pos] for i in range(n_layers)], color=color, label=f"Position {pos}", lw=0.1)

# Colorbar
sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=n_pos))
sm.set_array([])
plt.colorbar(sm, ticks=np.arange(0,n_pos+1,1), label="Position")
plt.title(f"Standard Deviation of Residual Stream activations after layer $N$ ({model_name})")
plt.xlabel("$N$")
plt.semilogy()
plt.ylabel("Standard Deviation")
plt.savefig(f"stds_by_pos_{model_name}.png")

# %% [markdown]
# #### Double check bump coing from paddding tokens
#
# The first few prompts have ~6 tokens so we should see exclusively padding later

# %%
n_pos = len(stds[0][0])
n_layers = len(stds)
plt.figure(figsize=(10,5))
for batch in range(5):
    for pos in range(n_pos):
        #print(f"Batch {batch}, Position {pos}")
        color = plt.cm.viridis(pos/n_pos)
        plt.plot(range(n_layers), [stds[i][batch][pos] for i in range(n_layers)], color=color, label=f"Position {pos}", lw=0.1)

# Colorbar
sm = plt.cm.ScalarMappable(cmap=plt.cm.viridis, norm=plt.Normalize(vmin=0, vmax=n_pos))
sm.set_array([])
plt.colorbar(sm, ticks=np.arange(0,n_pos+1,1), label="Position")
plt.title(f"Standard Deviation of Residual Stream activations after layer $N$ ({model_name})")
plt.xlabel("$N$")
plt.semilogy()
plt.ylabel("Standard Deviation")

# %% [markdown]
# ## Compare different models
#
# Note that switching between Standard Deviation and Norm shifts models slightly relative to each other, as they all have different residual stream sizes.

# %% [markdown]
# Norms:

# %%
# Norms
import os
for test_model_name in ["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                        "opt-125m", "opt-1.3b", "opt-2.7b",\
                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                        "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]:
    # Check if file exists
    if os.path.isfile(f"meannorms_post_{test_model_name}.npy"):
        continue
    test_model = HookedTransformer.from_pretrained(
        test_model_name, device="cpu"
    )
    cache = test_model.run_with_cache(prompts)[1]
    meannorms_pre = []
    meannorms_mid = []
    meannorms_post = []
    for layer in range(test_model.cfg.n_layers):
        #norm = cache[f"blocks.{layer}.hook_resid_post"].norm(dim=-1)
        norm_pre = cache[f"blocks.{layer}.hook_resid_pre"].norm(dim=-1)
        if "pythia" in test_model_name:
            norm_mid = norm_pre
        else:
            norm_mid = cache[f"blocks.{layer}.hook_resid_mid"].norm(dim=-1)
        norm_post = cache[f"blocks.{layer}.hook_resid_post"].norm(dim=-1)
        norms_pre_non_BOS_non_PAD = []
        norms_mid_non_BOS_non_PAD = []
        norms_post_non_BOS_non_PAD = []
        for batch in range(len(prompts)):
            prompt_len = len(test_model.to_str_tokens(prompts[batch]))
            for pos in range(1, prompt_len):
                norms_pre_non_BOS_non_PAD.append(norm_pre[batch, pos].item())
                norms_mid_non_BOS_non_PAD.append(norm_mid[batch, pos].item())
                norms_post_non_BOS_non_PAD.append(norm_post[batch, pos].item())
        meannorms_pre.append(torch.tensor(norms_pre_non_BOS_non_PAD).mean().item())
        meannorms_mid.append(torch.tensor(norms_mid_non_BOS_non_PAD).mean().item())
        meannorms_post.append(torch.tensor(norms_post_non_BOS_non_PAD).mean().item())
        #.append(torch.tensor(norms_non_BOS_non_PAD).mean().item())
    #np.save(f"meannorms_{test_model_name}.npy", meannorms)
    np.save(f"meannorms_pre_{test_model_name}.npy", meannorms_pre)
    np.save(f"meannorms_mid_{test_model_name}.npy", meannorms_mid)
    np.save(f"meannorms_post_{test_model_name}.npy", meannorms_post)

# %%
fig, [ax1, ax2] = plt.subplots(2, 1, figsize=(6,8), constrained_layout=True, sharey=True)
for i, test_model_name in enumerate(["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                                     "opt-125m", "opt-1.3b", "opt-2.7b",\
                                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                                            "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]):
    meanstds = np.load(f"meannorms_post_{test_model_name}.npy")
    ax = ax1 if i < 8 else ax2
    ax.semilogy(range(len(meanstds)), meanstds, marker="o", label=test_model_name, alpha=0.5)
    # Fit line (by eye)
    if test_model_name=="gpt2-xl":
        min_layer=5
        max_layer=41
        slope = (np.log10(meanstds[max_layer])-np.log10(meanstds[min_layer]))/(max_layer-min_layer)
        print(f"Factor per layer: {10**(slope):.3f}")
        ax.plot((min_layer, max_layer), [meanstds[i] for i in (min_layer, max_layer)], color="orange", label=f"gpt2-xl slope: {10**slope:.1%}", alpha=1)

fig.suptitle("Norm of Residual Stream activations after layer $N$")
ax1.set_title("OpenAI and Facebook models")
ax2.set_title("EleutherAI models")
ax1.set_xlabel("Layer index $N$")
ax2.set_xlabel("Layer index $N$")
ax1.set_ylabel("Residual stream norm")
ax2.set_ylabel("Residual stream norm")
ax2.set_xlim(ax1.get_xlim())
ax1.legend()
ax2.legend()
fig.savefig(f"norms_all.png")
plt.show()

# %% [markdown]
# Standard Deviations:

# %%
import os
for test_model_name in ["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                        "opt-125m", "opt-1.3b", "opt-2.7b",\
                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                        "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]:
    # Check if file exists
    if os.path.isfile(f"meanstds_post_{test_model_name}.npy"):
        continue
    test_model = HookedTransformer.from_pretrained(
        test_model_name, device="cpu"
    )
    cache = test_model.run_with_cache(prompts)[1]
    meanstds_pre = []
    meanstds_mid = []
    meanstds_post = []
    for layer in range(test_model.cfg.n_layers):
        #std = cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1)
        std_pre = cache[f"blocks.{layer}.hook_resid_pre"].std(dim=-1)
        if "pythia" in test_model_name:
            std_mid = std_pre
        else:
            std_mid = cache[f"blocks.{layer}.hook_resid_mid"].std(dim=-1)
        std_post = cache[f"blocks.{layer}.hook_resid_post"].std(dim=-1)
        stds_pre_non_BOS_non_PAD = []
        stds_mid_non_BOS_non_PAD = []
        stds_post_non_BOS_non_PAD = []
        for batch in range(len(prompts)):
            prompt_len = len(test_model.to_str_tokens(prompts[batch]))
            for pos in range(1, prompt_len):
                stds_pre_non_BOS_non_PAD.append(std_pre[batch, pos].item())
                stds_mid_non_BOS_non_PAD.append(std_mid[batch, pos].item())
                stds_post_non_BOS_non_PAD.append(std_post[batch, pos].item())
        meanstds_pre.append(torch.tensor(stds_pre_non_BOS_non_PAD).mean().item())
        meanstds_mid.append(torch.tensor(stds_mid_non_BOS_non_PAD).mean().item())
        meanstds_post.append(torch.tensor(stds_post_non_BOS_non_PAD).mean().item())
        #.append(torch.tensor(stds_non_BOS_non_PAD).mean().item())
    #np.save(f"meanstds_{test_model_name}.npy", meanstds)
    np.save(f"meanstds_pre_{test_model_name}.npy", meanstds_pre)
    np.save(f"meanstds_mid_{test_model_name}.npy", meanstds_mid)
    np.save(f"meanstds_post_{test_model_name}.npy", meanstds_post)

# %%
fig, [ax1, ax2] = plt.subplots(1, 2, figsize=(12,5), constrained_layout=True, sharey=True)
for i, test_model_name in enumerate(["distilgpt2", "gpt2-small", 'gpt2-medium', 'gpt2-large', 'gpt2-xl', \
                                     "opt-125m", "opt-1.3b", "opt-2.7b",\
                                        "gpt-neo-125M", "gpt-neo-1.3B", "gpt-neo-2.7B",\
                                            "pythia-70m", "pythia-160m", "pythia-410m", "pythia-1b", "pythia-1.4b", "pythia-2.8b"]):
    meanstds = np.load(f"meanstds_post_{test_model_name}.npy")
    ax = ax1 if i < 8 else ax2
    ax.semilogy(range(len(meanstds)), meanstds, marker="o", label=test_model_name, alpha=0.5)
    # Fit line (by eye)
    if test_model_name=="gpt2-xl":
        min_layer=5
        max_layer=41
        slope = (np.log10(meanstds[max_layer])-np.log10(meanstds[min_layer]))/(max_layer-min_layer)
        print(f"Factor per layer: {10**(slope):.3f}")
        # Default color
        c = ax.lines[-1].get_color()
        ax.plot((min_layer, max_layer), [meanstds[i] for i in (min_layer, max_layer)], color=c, label=f"gpt2-xl slope: {10**slope:.1%}", alpha=0.5)

fig.suptitle("Mean Standard Deviation of Residual Stream activations after layer $N$")
ax1.set_title("OpenAI and Facebook models")
ax2.set_title("EleutherAI models")
ax1.set_xlabel("Layer index $N$")
ax2.set_xlabel("Layer index $N$")
ax1.set_ylabel("Mean Standard Deviation")
ax2.set_ylabel("Mean Standard Deviation")
ax2.set_xlim(ax1.get_xlim())
ax1.legend()
ax2.legend()
fig.savefig(f"stds_all.png")
plt.show()

# %% [markdown]
# ## Analyze where the growth comes from
#
# In particular we can use the `resid_mid` hook to distinguish Attention and MLP

# %%
for i, test_model_name in enumerate(['gpt2-medium', 'gpt2-large', 'gpt2-xl']):
    fig, ax = plt.subplots(1, 1, figsize=(5,3), constrained_layout=True)
    meannorms_pre = np.load(f"meannorms_pre_{test_model_name}.npy")
    meannorms_mid = np.load(f"meannorms_mid_{test_model_name}.npy")
    meannorms_post = np.load(f"meannorms_post_{test_model_name}.npy")
    fig.suptitle("Contributions to residual stream norm growth in "+test_model_name)
    ax.plot(range(len(meannorms_pre)), (meannorms_mid/meannorms_pre), label="Attention")
    ax.plot(range(len(meannorms_pre)), (meannorms_post/meannorms_mid), label="MLP")
    ax.set_xlabel("Layer index $N$")
    ax.set_ylim(0.98, 1.10)
    ax.set_ylabel("Relative norm increase")
    ax.axhline(1, ls="--", color="k")
    ax.axhline(1.045, ls=":", color="k")
    fig.savefig(f"contributions_split_{test_model_name}.png")
    ax.legend()
plt.show()

# %% [markdown]
# ### Analyze W_OV matricies
#
# We can either
# 1. Use the Frobenius norm of the W_OV matrices, which is equivalent to "How much would this matrix change the norm of a random input", or
# 2. Feed-in random actual embeddings to see if their effect on actual embeddings differs from that on random vectors
#
# for now try 1:

# %%
from fancy_einsum import einsum
print("Model name:", model_name)

df_OV_scale = pd.DataFrame(columns=["Layer", "Head", "Norm increase"])

for layer in range(model.cfg.n_layers):
    print(f"Layer {layer} of {model.cfg.n_layers}")
    W_OVs = einsum("head hidden embedout, head embed hidden -> head embed embedout", model.blocks[layer].attn.W_O, model.blocks[layer].attn.W_V)
    random_embed = torch.randn(1000, model.cfg.d_model).to(device)
    random_embed /= random_embed.std(dim=-1, keepdim=True)
    OV_output = torch.zeros(1000, model.cfg.n_heads+1, model.cfg.d_model).to(device)
    OV_output[:, :model.cfg.n_heads, :] = einsum("batch embed, head embed embedout -> batch head embedout", random_embed, W_OVs)
    OV_output[model.cfg.n_heads] = model.blocks[layer].attn.b_O.view(1,1,-1)
    mean_norm_increase = OV_output.std(dim=-1).mean(dim=0)
    norm_increase_analytic = W_OVs.norm(dim=(-2, -1))/np.sqrt(model.cfg.d_model)
    # Double check that Frobenius norm == actual random input effect
    assert torch.allclose(mean_norm_increase[:model.cfg.n_heads], norm_increase_analytic, rtol=0.2), (mean_norm_increase, norm_increase_analytic)
    total_norm_increase = OV_output.sum(dim=1).norm(dim=0)
    mean_total_norm_increase = total_norm_increase.mean(dim=0)
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Sum", mean_total_norm_increase.item()]], columns=["Layer", "Head", "Norm increase"])], ignore_index=True)
    df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, "Bias", mean_norm_increase[model.cfg.n_heads].item()]], columns=["Layer", "Head", "Norm increase"])], ignore_index=True)
    for head in range(model.cfg.n_heads):
        df_OV_scale = pd.concat([df_OV_scale, pd.DataFrame([[layer, head, norm_increase_analytic[head].item()]], columns=["Layer", "Head", "Norm increase"])], ignore_index=True)

fig = px.line(df_OV_scale, x="Layer", y="Norm increase", color="Head", log_y=True, title="Attention W_OV norms")
fig.show()

# %% [markdown]
# ### MLPs
#
# Norm non-trivial due to ReLU (correlated with a bunch of stuff). Try
# 1. the "input random vectors" strategy from above, or if that fails
# 2. the "input actual embeddings" strategy.

# %%

# %%
from fancy_einsum import einsum

print("Model name:", model.cfg.model_name)
cache = model.run_with_cache(prompts)[1]

df_MLP_scale = pd.DataFrame(columns=["Layer", "Norm increase", "Source"])
ReLU_zero_rates = {}

for layer in range(model.cfg.n_layers):
    pos = 3
    random_embed = cache[f"blocks.{layer}.hook_resid_mid"][:, pos:pos+1, :]
    # Not working with random embed right now
    #random_embed = torch.randn_like(random_embed)
    random_embed /= random_embed.std(dim=-1, keepdim=True)
    assert torch.allclose(torch.tensor(0.), random_embed.mean(dim=-1), atol=1e-5), random_embed
    mlp_out = model.blocks[layer].mlp(random_embed)
    mlp_out += random_embed
    assert torch.allclose(torch.tensor(0.), mlp_out.mean(dim=-1), atol=1e-5), mlp_out
    norm_increase = mlp_out[:, 0, :].std(dim=-1)
    mean_norm_increase = norm_increase.mean(dim=0)
    df_MLP_scale = pd.concat(
        [
            df_MLP_scale,
            pd.DataFrame(
                [[layer, mean_norm_increase.item(), "Empirical"]],
                columns=["Layer", "Norm increase", "Source"],
            ),
        ],
        ignore_index=True,
    )
    hidden = (
        einsum(
            "batch pos embed, embed hidden -> batch pos hidden",
            random_embed,
            model.blocks[layer].mlp.W_in,
        )
        + model.blocks[layer].mlp.b_in
    )
    ReLU_zero_rate = (hidden[:, 0, :] < 0).float().mean()
    ReLU_zero_rates[layer] = ReLU_zero_rate
    df_MLP_scale = pd.concat(
        [
            df_MLP_scale,
            pd.DataFrame(
                [[layer, ReLU_zero_rate.item(), "dead-fraction"]],
                columns=["Layer", "Norm increase", "Source"],
            ),
        ],
        ignore_index=True,
    )

# Scatter Layer scale, log scale
fig = px.scatter(
    df_MLP_scale[df_MLP_scale.Source=="Empirical"],
    x="Layer",
    y="Norm increase",
    color="Source",
    log_y=True,
    title=f"How much the MLP increase the norm of the input, by layer (test with pos={pos})",
)
fig.show()

plt.plot(df_MLP_scale[df_MLP_scale.Source=="Empirical"]["Norm increase"], label="Empirical")

# %%
N = len(df_MLP_scale[df_MLP_scale.Source=="Real"]["Norm increase"])
plt.semilogy(range(N), df_MLP_scale[df_MLP_scale.Source=="Real"]["Norm increase"], label="Real", marker="o")
# Slope 5 to 41

min_layer=5
max_layer=41
slope = (np.log10(df_MLP_scale[df_MLP_scale.Source=="Real"]["Norm increase"].iloc[max_layer])-np.log10(df_MLP_scale[df_MLP_scale.Source=="Real"]["Norm increase"].iloc[min_layer]))/(max_layer-min_layer)
plt.semilogy((min_layer, max_layer), [df_MLP_scale[df_MLP_scale.Source=="Real"]["Norm increase"].iloc[i] for i in (min_layer, max_layer)], label=f"Real slope: {10**slope:.1%}", marker="o")
plt.legend()


# %% [markdown]
# ## Plotting residual stream magnitudes against layer number
# Let's zoom in on how specific token magnitudes evolve over a forward
# pass. It turns out that the zeroth position (the `<|endoftext|>` token) has a much larger
# magnitude than the rest. (This possibly explains the outlier
# magnitudes for the prompt histograms.)

# %%
def line_plot(
    df: pd.DataFrame,
    log_y: bool = True,
    title: str = "Residual Stream Norm by Layer Number",
    legend_title_text: str = "Prompt",
) -> go.Figure:
    """Make a line plot of the RichPrompt norm. If log_y is True,
    adds a column to the dataframe with the log10 of the norm."""
    for col in ["Prompt", "Activation Location", "Magnitude"]:
        assert col in df.columns, f"Column {col} not in dataframe"

    if log_y:
        df["LogMagnitude"] = np.log10(df["Magnitude"])

    fig = px.line(
        df,
        x="Activation Location",
        y="LogMagnitude" if log_y else "Magnitude",
        color="Prompt",
        color_discrete_sequence=px.colors.sequential.Rainbow[::-1],
    )

    fig.update_layout(
        legend_title_text=legend_title_text,
        title=title,
        xaxis_title="Layer Number",
        yaxis_title=f"Norm{' (log 10)' if log_y else ''}",
    )

    return fig



# %%
# Create an empty dataframe with the required columns
all_resid_pre_locations: List[int] = torch.arange(0, num_layers, 1).tolist()
addition_df = pd.DataFrame(columns=DF_COLS)

# Loop through activation locations and prompts
for act_loc in all_resid_pre_locations:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)

    for context in ("MATS is really cool",):
        mags: torch.Tensor = hook_utils.prompt_magnitudes(
            model=model, prompt=context, act_name=act_name
        ).cpu()
        str_tokens: List[str] = model.to_str_tokens(context)

        for pos, mag in enumerate(mags):
            # Create a new dataframe row with the current data
            row = pd.DataFrame(
                {
                    "Prompt": [str_tokens[pos]],
                    "Activation Location": [act_loc],
                    "Activation Name": [act_name],
                    "Magnitude": [mag],
                }
            )

            # Append the new row to the dataframe
            addition_df = pd.concat([addition_df, row], ignore_index=True)


# %%
for use_log in (True, False):
    fig = line_plot(
        addition_df,
        log_y=use_log,
        title=f"Residual Stream Norm by Layer Number in {model_name}",
    )
    fig.update_layout(width=600)
    fig.show()

# %% [markdown]
# To confirm the exponential increase in magnitude, let's plot the
# Frobenius
# norm of the residual stream at position `i` just before layer `t`,
# divided by the norm before `t-1`.

# %%
# Make a plotly line plot of the relative magnitudes vs layer
# number, with color representing the token location of the "MATS is
# really cool" prompt

# Create an empty dataframe with the required columns
all_resid_pre_locations: List[int] = torch.arange(1, num_layers, 1).tolist()
relative_df = pd.DataFrame(columns=DF_COLS)
MATS_prompt: str = "MATS is really cool"

mags_prev: torch.Tensor = hook_utils.prompt_magnitudes(
    model=model, prompt=MATS_prompt, act_name=prompt_utils.get_block_name(0)
).cpu()

# Loop through activation locations and prompts
for act_loc in all_resid_pre_locations:
    act_name: str = prompt_utils.get_block_name(block_num=act_loc)
    mags: torch.Tensor = hook_utils.prompt_magnitudes(
        model=model, prompt=MATS_prompt, act_name=act_name
    ).cpu()

    tokens: List[str] = model.to_str_tokens(MATS_prompt)
    for pos, mag in enumerate(mags):
        # Create a new dataframe row with the current data
        row = pd.DataFrame(
            {
                "Prompt": [tokens[pos]],
                "Activation Location": [act_loc],
                "Activation Name": [act_name],
                "Magnitude": [mag / mags_prev[pos]],
            }
        )

        # Append the new row to the dataframe
        relative_df = pd.concat([relative_df, row], ignore_index=True)

    mags_prev = mags


# %%
relative_fig = line_plot(
    relative_df,
    log_y=False,
    title=f"Norm(n)/Norm(n-1) across layers n in {model_name}",
    legend_title_text="Token",
)

# Set y label to be "Norm growth rate"
relative_fig.update_yaxes(title_text="Norm growth rate")

# Set y bounds to [.9, 1.5]
relative_fig.update_yaxes(range=[0.9, 1.5])

# Plot a horizontal line at y=1
relative_fig.add_hline(y=1, line_dash="dash", line_color="black")
relative_fig.update_layout(width=600)

relative_fig.show()
